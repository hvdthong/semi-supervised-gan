{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "import random\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_seed = 86347917\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(manual_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVHN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SvhnDataset(Dataset):\n",
    "    def __init__(self, image_size, split):\n",
    "        self.split = split\n",
    "        self.use_gpu = True if torch.cuda.is_available() else False\n",
    "\n",
    "        self.svhn_dataset = self._create_dataset(image_size, split)\n",
    "        self.label_mask = self._create_label_mask()\n",
    "\n",
    "    def _create_dataset(self, image_size, split):\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5],\n",
    "            std=[0.5, 0.5, 0.5])\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            normalize])\n",
    "        return datasets.SVHN(root='./svhn', download=True, transform=transform, split=split)\n",
    "\n",
    "    def _is_train_dataset(self):\n",
    "        return True if self.split == 'train' else False\n",
    "\n",
    "    def _create_label_mask(self):\n",
    "        if self._is_train_dataset():\n",
    "            label_mask = np.zeros(len(self.svhn_dataset))\n",
    "            label_mask[0:1000] = 1\n",
    "            np.random.shuffle(label_mask)\n",
    "            label_mask = torch.LongTensor(label_mask)\n",
    "            return label_mask\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.svhn_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.svhn_dataset.__getitem__(idx)\n",
    "        if self._is_train_dataset():\n",
    "            return data, label, self.label_mask[idx]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(image_size, batch_size):\n",
    "    num_workers = 2\n",
    "\n",
    "    svhn_train = SvhnDataset(image_size=image_size, split='train')\n",
    "    svhn_test = SvhnDataset(image_size=image_size, split='test')\n",
    "\n",
    "    svhn_loader_train = DataLoader(\n",
    "        dataset=svhn_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    svhn_loader_test = DataLoader(\n",
    "        dataset=svhn_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return svhn_loader_train, svhn_loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./svhn\\train_32x32.mat\n",
      "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./svhn\\test_32x32.mat\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-14bac31c4ad6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msvhn_loader_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m36\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimage_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvhn_loader_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_iter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                 \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m  \u001b[1;31m# ensure that the worker exits on process exit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m                 \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0m_update_worker_pids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "svhn_loader_train, _ = get_loader(image_size=32, batch_size=36)\n",
    "image_iter = iter(svhn_loader_train)\n",
    "images, _, _ = image_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_images(images):\n",
    "    assert(len(images) >= 36)\n",
    "    fig, axes = plt.subplots(6, 6, sharex=True, sharey=True, figsize=(5,5))\n",
    "    for idx, ax in enumerate(axes.flatten()):\n",
    "        img = images[idx].numpy()\n",
    "        img = img.transpose(1, 2, 0)\n",
    "        img = ((img - img.min())*255 / (img.max() - img.min())).astype(np.uint8)\n",
    "        ax.imshow(img, aspect='equal')\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ganLogits(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(_ganLogits, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, class_logits):\n",
    "        real_class_logits, fake_class_logits = torch.split(class_logits, self.num_classes, dim=1)\n",
    "        fake_class_logits = torch.squeeze(fake_class_logits)\n",
    "        \n",
    "        max_val, _ = torch.max(real_class_logits, 1, keepdim=True)\n",
    "        stable_class_logits = real_class_logits - max_val\n",
    "        max_val = torch.squeeze(max_val)\n",
    "        gan_logits = torch.log(torch.sum(torch.exp(stable_class_logits), 1)) + max_val - fake_class_logits\n",
    "        \n",
    "        return gan_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netG(nn.Module):\n",
    "    def __init__(self, nz, ngf, alpha, nc, use_gpu):\n",
    "        super(_netG, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # noise is going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ngf * 4) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ngf * 2) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ngf) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # (nc) x 32 x 32\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if isinstance(inputs.data, torch.cuda.FloatTensor) and self.use_gpu:\n",
    "            out = nn.parallel.data_parallel(self.main, inputs, range(1))\n",
    "        else:\n",
    "            out = self.main(inputs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netD(nn.Module):\n",
    "    def __init__(self, ndf, alpha, nc, drop_rate, num_classes, use_gpu):\n",
    "        super(_netD, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Dropout2d(drop_rate/2.5),\n",
    "            \n",
    "            # input is (number_channels) x 32 x 32\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            nn.Dropout2d(drop_rate),\n",
    "            # (ndf) x 16 x 16\n",
    "            nn.Conv2d(ndf, ndf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ndf) x 8 x 8\n",
    "            nn.Conv2d(ndf, ndf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            nn.Dropout2d(drop_rate),\n",
    "            # (ndf) x 4 x 4\n",
    "            nn.Conv2d(ndf, ndf * 2, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ndf * 2) x 4 x 4\n",
    "            nn.Conv2d(ndf * 2, ndf * 2, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ndf * 2) x 4 x 4\n",
    "            nn.Conv2d(ndf * 2, ndf * 2, 3, 1, 0, bias=False),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ndf * 2) x 2 x 2\n",
    "        )\n",
    "        \n",
    "        self.features = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "        self.class_logits = nn.Linear(\n",
    "            in_features=(ndf * 2) * 1 * 1,\n",
    "            out_features=num_classes + 1)\n",
    "        \n",
    "        self.gan_logits = _ganLogits(num_classes)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if isinstance(inputs.data, torch.cuda.FloatTensor) and self.use_gpu:\n",
    "            out = nn.parallel.data_parallel(self.main, inputs, range(1))\n",
    "        else:\n",
    "            out = self.main(inputs)\n",
    "\n",
    "        features = self.features(out)\n",
    "        features = features.squeeze()\n",
    "\n",
    "        class_logits = self.class_logits(features)\n",
    "\n",
    "        gan_logits = self.gan_logits(class_logits)\n",
    "        \n",
    "        out = self.softmax(class_logits)\n",
    "\n",
    "        return out, class_logits, gan_logits, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings:\n",
    "    def __init__(self):\n",
    "        self.image_size = 32\n",
    "        self.batch_size = 64\n",
    "        self.epochs = 80\n",
    "        self.nz = 100\n",
    "        self.nc = 3\n",
    "        self.alpha = .2\n",
    "        self.drop_rate = .5\n",
    "        self.ngf = 64\n",
    "        self.ndf = 64\n",
    "        self.learning_rate = .0002\n",
    "        self.beta1 = .5\n",
    "        self.out_dir = './train_out'\n",
    "        self.best_netG_filename = '{}/netG_best.pth'.format(self.out_dir)\n",
    "        self.best_netD_filename = '{}/netD_best.pth'.format(self.out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class Solver:\n",
    "    def __init__(self, svhn_loader_train, svhn_loader_test, settings):\n",
    "        self.settings = settings\n",
    "        self.use_gpu = True if torch.cuda.is_available() else False\n",
    "        self.num_classes = 10\n",
    "        self.svhn_loader_train = svhn_loader_train\n",
    "        self.svhn_loader_test = svhn_loader_test\n",
    "        \n",
    "        self.netG, self.netD = self._build_model()\n",
    "        self.g_optimizer, self.d_optimizer = self._create_optimizers()\n",
    "        # , self.g_lr_scheduler, self.d_lr_scheduler \n",
    "\n",
    "    def _build_model(self):\n",
    "        netG = _netG(\n",
    "            self.settings.nz, self.settings.ngf, self.settings.alpha,\n",
    "            self.settings.nc, self.use_gpu)\n",
    "        netG.apply(self._weights_init)\n",
    "        print(netG)\n",
    "        \n",
    "        netD = _netD(\n",
    "            self.settings.ndf, self.settings.alpha, self.settings.nc,\n",
    "            self.settings.drop_rate, self.num_classes, self.use_gpu)\n",
    "        netD.apply(self._weights_init)\n",
    "        print(netD)\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            netG = netG.cuda()\n",
    "            netD = netD.cuda()\n",
    "\n",
    "        return netG, netD\n",
    "\n",
    "    def _weights_init(self, module):\n",
    "        classname = module.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            module.weight.data.normal_(0.0, 0.02)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            module.weight.data.normal_(1.0, 0.02)\n",
    "            module.bias.data.fill_(0)\n",
    "\n",
    "    def _create_optimizers(self):\n",
    "        g_params = list(self.netG.parameters())\n",
    "        d_params = list(self.netD.parameters())\n",
    "\n",
    "        g_optimizer = optim.Adam(g_params, self.settings.learning_rate, betas=(self.settings.beta1, 0.999))\n",
    "        d_optimizer = optim.Adam(d_params, self.settings.learning_rate, betas=(self.settings.beta1, 0.999))\n",
    "        # g_lr_scheduler = optim.lr_scheduler.StepLR(g_optimizer, step_size=1, gamma=0.9)\n",
    "        # d_lr_scheduler = optim.lr_scheduler.StepLR(d_optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "        return g_optimizer, d_optimizer #, g_lr_scheduler, d_lr_scheduler\n",
    "\n",
    "    def _to_var(self, x):\n",
    "        if self.use_gpu:\n",
    "            x = x.cuda()\n",
    "        return Variable(x)\n",
    "\n",
    "    def _one_hot(self, x):\n",
    "        label_numpy = x.data.cpu().numpy()\n",
    "        label_onehot = np.zeros((label_numpy.shape[0], self.num_classes + 1))\n",
    "        label_onehot[np.arange(label_numpy.shape[0]), label_numpy] = 1\n",
    "        label_onehot = self._to_var(torch.FloatTensor(label_onehot))\n",
    "        return label_onehot\n",
    "    \n",
    "    def _reset_grad(self):\n",
    "        self.g_optimizer.zero_grad()\n",
    "        self.d_optimizer.zero_grad()\n",
    "\n",
    "    def load_model(self, netG_filename, netD_filename):\n",
    "        self.netG.load_state_dict(torch.load(netG_filename))\n",
    "        self.netD.load_state_dict(torch.load(netD_filename))\n",
    "        \n",
    "    def test(self, epoch_num, epochs):\n",
    "        correct = 0\n",
    "        num_samples = 0\n",
    "        self.netD.eval()\n",
    "        \n",
    "        for i, data in enumerate(self.svhn_loader_test):\n",
    "            svhn_data, svhn_labels = data\n",
    "            svhn_data = self._to_var(svhn_data)\n",
    "            svhn_labels = self._to_var(svhn_labels).long().squeeze()\n",
    "\n",
    "            d_out, _, _, _ = self.netD(svhn_data)\n",
    "            _, pred_idx = torch.max(d_out.data, 1)\n",
    "            eq = torch.eq(svhn_labels.data, pred_idx)\n",
    "            correct += torch.sum(eq.float())\n",
    "            num_samples += len(svhn_labels)\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print('Test:\\tepoch {}/{}\\tsamples {}/{}'.format(\n",
    "                    epoch_num, epochs, i + 1, len(self.svhn_loader_test)))\n",
    "\n",
    "        accuracy = correct/max(1.0, 1.0 * num_samples)\n",
    "        print('Test:\\tepoch {}/{}\\taccuracy {}'.format(epoch_num, epochs, accuracy))\n",
    "        \n",
    "    def train(self):\n",
    "        if not os.path.exists(self.settings.out_dir):\n",
    "            os.makedirs(self.settings.out_dir)\n",
    "        \n",
    "        d_gan_criterion = nn.BCEWithLogitsLoss()\n",
    "        d_gan_class_criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        noise = torch.FloatTensor(self.settings.batch_size, self.settings.nz, 1, 1)\n",
    "        \n",
    "        fixed_noise = torch.FloatTensor(self.settings.batch_size, self.settings.nz, 1, 1).normal_(0, 1)\n",
    "        fixed_noise = self._to_var(fixed_noise)\n",
    "        \n",
    "        d_gan_labels_real = torch.LongTensor(self.settings.batch_size)\n",
    "        d_gan_labels_fake = torch.LongTensor(self.settings.batch_size)\n",
    "        \n",
    "        best_accuracy = 0.0\n",
    "\n",
    "        for epoch in range(1, self.settings.epochs + 1):\n",
    "            masked_correct = 0\n",
    "            num_samples = 0\n",
    "            \n",
    "            self.netD.train()\n",
    "            self.netG.train()\n",
    "            for i, data in enumerate(self.svhn_loader_train):\n",
    "                svhn_data, svhn_labels, label_mask = data\n",
    "                svhn_data = self._to_var(svhn_data)\n",
    "                svhn_labels = self._to_var(svhn_labels).long().squeeze()\n",
    "                label_mask = self._to_var(label_mask).float().squeeze()\n",
    "\n",
    "                # -------------- train netD --------------\n",
    "\n",
    "                self._reset_grad()\n",
    "                \n",
    "                # train with real images\n",
    "                # d_out == softmax(d_class_logits)\n",
    "                d_out, d_class_logits_on_data, d_gan_logits_real, d_sample_features = self.netD(svhn_data)\n",
    "                d_gan_labels_real.resize_as_(svhn_labels.data.cpu()).fill_(1)\n",
    "                d_gan_labels_real_var = self._to_var(d_gan_labels_real).float()\n",
    "                d_gan_loss_real = d_gan_criterion(\n",
    "                    d_gan_logits_real,\n",
    "                    d_gan_labels_real_var)\n",
    "                \n",
    "                # train with fake images\n",
    "                noise.resize_(svhn_labels.data.shape[0], self.settings.nz, 1, 1).normal_(0, 1)\n",
    "                noise_var = self._to_var(noise)\n",
    "                fake = self.netG(noise_var)\n",
    "\n",
    "                # call detach() to avoid backprop for netG here\n",
    "                _, _, d_gan_logits_fake, _ = self.netD(fake.detach())\n",
    "                d_gan_labels_fake.resize_(svhn_labels.data.shape[0]).fill_(0)\n",
    "                d_gan_labels_fake_var = self._to_var(d_gan_labels_fake).float()\n",
    "                d_gan_loss_fake = d_gan_criterion(\n",
    "                    d_gan_logits_fake,\n",
    "                    d_gan_labels_fake_var)\n",
    "\n",
    "                d_gan_loss = d_gan_loss_real + d_gan_loss_fake\n",
    "\n",
    "                # d_out == softmax(d_class_logits)\n",
    "                # see https://stackoverflow.com/questions/34240703/whats-the-difference-between-softmax-and-softmax-cross-entropy-with-logits/39499486#39499486\n",
    "                svhn_labels_one_hot = self._one_hot(svhn_labels)\n",
    "                d_class_loss_entropy = -torch.sum(svhn_labels_one_hot * torch.log(d_out), dim=1)\n",
    "                \n",
    "                d_class_loss_entropy = d_class_loss_entropy.squeeze()\n",
    "                delim = torch.max(torch.Tensor([1.0, torch.sum(label_mask.data)]))\n",
    "                d_class_loss = torch.sum(label_mask * d_class_loss_entropy) / delim\n",
    "                # numpy_labels = svhn_labels.data.cpu().numpy()\n",
    "                # delim = torch.Tensor(numpy_labels.shape[0])\n",
    "                # d_class_loss = torch.sum(d_class_loss_entropy) / delim # DO THIS FOR QUICK TEST ONLY!!!\n",
    "                \n",
    "                d_loss = d_gan_loss + d_class_loss\n",
    "                \n",
    "                d_loss.backward(retain_graph=True)\n",
    "                self.d_optimizer.step()\n",
    "\n",
    "                # -------------- update netG --------------\n",
    "                \n",
    "                self._reset_grad()\n",
    "                \n",
    "                # call netD again to do backprop for netG here\n",
    "                _, _, _, d_data_features = self.netD(fake)\n",
    "                \n",
    "                # Here we set `g_loss` to the \"feature matching\" loss invented by Tim Salimans at OpenAI.\n",
    "                # This loss consists of minimizing the absolute difference between the expected features\n",
    "                # on the data and the expected features on the generated samples.\n",
    "                # This loss works better for semi-supervised learning than the tradition GAN losses.\n",
    "                data_features_mean = torch.mean(d_data_features, dim=0).squeeze()\n",
    "                sample_features_mean = torch.mean(d_sample_features, dim=0).squeeze()\n",
    "                \n",
    "                g_loss = torch.mean(torch.abs(data_features_mean - sample_features_mean))\n",
    "\n",
    "                g_loss.backward()\n",
    "                self.g_optimizer.step()\n",
    "\n",
    "                _, pred_class = torch.max(d_class_logits_on_data, 1)\n",
    "                eq = torch.eq(svhn_labels, pred_class)\n",
    "                correct = torch.sum(eq.float())\n",
    "                masked_correct += torch.sum(label_mask * eq.float())\n",
    "                # masked_correct += correct\n",
    "                num_samples += torch.sum(label_mask)\n",
    "                # num_samples += numpy_labels.shape[0]\n",
    "                \n",
    "                if i % 200 == 0:\n",
    "                    print('Training:\\tepoch {}/{}\\tdiscr. gan loss {}\\tdiscr. class loss {}\\tgen loss {}\\tsamples {}/{}'.\n",
    "                        format(epoch, self.settings.epochs, d_gan_loss.data[0], d_class_loss.data[0], g_loss.data[0], \n",
    "                               i + 1, len(self.svhn_loader_train)))\n",
    "                    real_cpu, _, _ = data\n",
    "                    vutils.save_image(real_cpu,\n",
    "                            '%s/real_samples.png' % self.settings.out_dir,\n",
    "                            normalize=True)\n",
    "                    fake = self.netG(fixed_noise)\n",
    "                    vutils.save_image(fake.data,\n",
    "                            '{}/fake_samples_epoch_{:03d}.png'.format(self.settings.out_dir, epoch),\n",
    "                            normalize=True)\n",
    "                    \n",
    "            accuracy = masked_correct.data[0]/max(1.0, num_samples.data[0])\n",
    "            print('Training:\\tepoch {}/{}\\taccuracy {}'.format(epoch, self.settings.epochs, accuracy))\n",
    "\n",
    "            self.test(epoch, self.settings.epochs)\n",
    "\n",
    "            # do checkpointing\n",
    "            netG_filename = '{}/netG_epoch_{}.pth'.format(self.settings.out_dir, epoch)\n",
    "            netD_filename = '{}/netD_epoch_{}.pth'.format(self.settings.out_dir, epoch)\n",
    "            torch.save(self.netG.state_dict(), netG_filename)\n",
    "            torch.save(self.netD.state_dict(), netD_filename)\n",
    "            \n",
    "            # decay learning rate\n",
    "            # self.g_lr_scheduler.step()\n",
    "            # self.d_lr_scheduler.step()\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                shutil.copyfile(netG_filename, self.settings.best_netG_filename)\n",
    "                shutil.copyfile(netD_filename, self.settings.best_netD_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./svhn/train_32x32.mat\n",
      "Using downloaded and verified file: ./svhn/test_32x32.mat\n",
      "_netG(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d (100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): LeakyReLU(0.2)\n",
      "    (3): ConvTranspose2d (256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (5): LeakyReLU(0.2)\n",
      "    (6): ConvTranspose2d (128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (8): LeakyReLU(0.2)\n",
      "    (9): ConvTranspose2d (64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): Tanh()\n",
      "  )\n",
      ")\n",
      "_netD(\n",
      "  (main): Sequential(\n",
      "    (0): Dropout2d(p=0.2)\n",
      "    (1): Conv2d (3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (2): LeakyReLU(0.2)\n",
      "    (3): Dropout2d(p=0.5)\n",
      "    (4): Conv2d (64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (6): LeakyReLU(0.2)\n",
      "    (7): Conv2d (64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (9): LeakyReLU(0.2)\n",
      "    (10): Dropout2d(p=0.5)\n",
      "    (11): Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (13): LeakyReLU(0.2)\n",
      "    (14): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (16): LeakyReLU(0.2)\n",
      "    (17): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "    (18): LeakyReLU(0.2)\n",
      "  )\n",
      "  (features): AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  (class_logits): Linear(in_features=128, out_features=11)\n",
      "  (gan_logits): _ganLogits(\n",
      "  )\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "settings = Settings()\n",
    "svhn_loader_train, svhn_loader_test = get_loader(settings.image_size, settings.batch_size)\n",
    "solver = Solver(svhn_loader_train, svhn_loader_test, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\tepoch 1/80\tdiscr. gan loss 2.5888805389404297\tdiscr. class loss 4.103990077972412\tgen loss 0.031043389812111855\tsamples 1/1145\n",
      "Training:\tepoch 1/80\tdiscr. gan loss 1.4873476028442383\tdiscr. class loss 0.0\tgen loss 0.015703300014138222\tsamples 201/1145\n",
      "Training:\tepoch 1/80\tdiscr. gan loss 1.3721601963043213\tdiscr. class loss 4.532161235809326\tgen loss 0.01605299673974514\tsamples 401/1145\n",
      "Training:\tepoch 1/80\tdiscr. gan loss 1.3481419086456299\tdiscr. class loss 4.137684345245361\tgen loss 0.023889586329460144\tsamples 601/1145\n",
      "Training:\tepoch 1/80\tdiscr. gan loss 1.3292031288146973\tdiscr. class loss 0.0\tgen loss 0.03308148309588432\tsamples 801/1145\n",
      "Training:\tepoch 1/80\tdiscr. gan loss 1.3252284526824951\tdiscr. class loss 4.092351913452148\tgen loss 0.016248084604740143\tsamples 1001/1145\n",
      "Training:\tepoch 1/80\taccuracy 0.0\n",
      "Test:\tepoch 1/80\tsamples 1/407\n",
      "Test:\tepoch 1/80\tsamples 51/407\n",
      "Test:\tepoch 1/80\tsamples 101/407\n",
      "Test:\tepoch 1/80\tsamples 151/407\n",
      "Test:\tepoch 1/80\tsamples 201/407\n",
      "Test:\tepoch 1/80\tsamples 251/407\n",
      "Test:\tepoch 1/80\tsamples 301/407\n",
      "Test:\tepoch 1/80\tsamples 351/407\n",
      "Test:\tepoch 1/80\tsamples 401/407\n",
      "Test:\tepoch 1/80\taccuracy 0.08232175783650891\n",
      "Training:\tepoch 2/80\tdiscr. gan loss 1.3405673503875732\tdiscr. class loss 4.079894542694092\tgen loss 0.021092282608151436\tsamples 1/1145\n",
      "Training:\tepoch 2/80\tdiscr. gan loss 1.4114197492599487\tdiscr. class loss 4.266972541809082\tgen loss 0.012248882092535496\tsamples 201/1145\n",
      "Training:\tepoch 2/80\tdiscr. gan loss 1.4476420879364014\tdiscr. class loss 0.0\tgen loss 0.01163563597947359\tsamples 401/1145\n",
      "Training:\tepoch 2/80\tdiscr. gan loss 1.3750495910644531\tdiscr. class loss 0.0\tgen loss 0.010085511021316051\tsamples 601/1145\n",
      "Training:\tepoch 2/80\tdiscr. gan loss 1.4119378328323364\tdiscr. class loss 0.0\tgen loss 0.011714307591319084\tsamples 801/1145\n",
      "Training:\tepoch 2/80\tdiscr. gan loss 1.3524374961853027\tdiscr. class loss 4.552323818206787\tgen loss 0.011794577352702618\tsamples 1001/1145\n",
      "Training:\tepoch 2/80\taccuracy 0.0\n",
      "Test:\tepoch 2/80\tsamples 1/407\n",
      "Test:\tepoch 2/80\tsamples 51/407\n",
      "Test:\tepoch 2/80\tsamples 101/407\n",
      "Test:\tepoch 2/80\tsamples 151/407\n",
      "Test:\tepoch 2/80\tsamples 201/407\n",
      "Test:\tepoch 2/80\tsamples 251/407\n",
      "Test:\tepoch 2/80\tsamples 301/407\n",
      "Test:\tepoch 2/80\tsamples 351/407\n",
      "Test:\tepoch 2/80\tsamples 401/407\n",
      "Test:\tepoch 2/80\taccuracy 0.09780270436385986\n",
      "Training:\tepoch 3/80\tdiscr. gan loss 1.3658802509307861\tdiscr. class loss 0.0\tgen loss 0.009266435168683529\tsamples 1/1145\n",
      "Training:\tepoch 3/80\tdiscr. gan loss 1.4436473846435547\tdiscr. class loss 4.4591064453125\tgen loss 0.010079997591674328\tsamples 201/1145\n",
      "Training:\tepoch 3/80\tdiscr. gan loss 1.4168274402618408\tdiscr. class loss 4.1311774253845215\tgen loss 0.0194823257625103\tsamples 401/1145\n",
      "Training:\tepoch 3/80\tdiscr. gan loss 1.3405005931854248\tdiscr. class loss 4.272391319274902\tgen loss 0.019578712061047554\tsamples 601/1145\n",
      "Training:\tepoch 3/80\tdiscr. gan loss 1.3010798692703247\tdiscr. class loss 0.0\tgen loss 0.022626664489507675\tsamples 801/1145\n",
      "Training:\tepoch 3/80\tdiscr. gan loss 1.4241724014282227\tdiscr. class loss 0.0\tgen loss 0.034349530935287476\tsamples 1001/1145\n",
      "Training:\tepoch 3/80\taccuracy 0.0\n",
      "Test:\tepoch 3/80\tsamples 1/407\n",
      "Test:\tepoch 3/80\tsamples 51/407\n",
      "Test:\tepoch 3/80\tsamples 101/407\n",
      "Test:\tepoch 3/80\tsamples 151/407\n",
      "Test:\tepoch 3/80\tsamples 201/407\n",
      "Test:\tepoch 3/80\tsamples 251/407\n",
      "Test:\tepoch 3/80\tsamples 301/407\n",
      "Test:\tepoch 3/80\tsamples 351/407\n",
      "Test:\tepoch 3/80\tsamples 401/407\n",
      "Test:\tepoch 3/80\taccuracy 0.07402427781192379\n",
      "Training:\tepoch 4/80\tdiscr. gan loss 1.3225183486938477\tdiscr. class loss 4.127772808074951\tgen loss 0.011614379473030567\tsamples 1/1145\n",
      "Training:\tepoch 4/80\tdiscr. gan loss 1.360813021659851\tdiscr. class loss 4.094714164733887\tgen loss 0.019542917609214783\tsamples 201/1145\n",
      "Training:\tepoch 4/80\tdiscr. gan loss 1.3114542961120605\tdiscr. class loss 0.0\tgen loss 0.028275253251194954\tsamples 401/1145\n",
      "Training:\tepoch 4/80\tdiscr. gan loss 1.239670991897583\tdiscr. class loss 0.0\tgen loss 0.025930238887667656\tsamples 601/1145\n",
      "Training:\tepoch 4/80\tdiscr. gan loss 1.3791035413742065\tdiscr. class loss 0.0\tgen loss 0.034651026129722595\tsamples 801/1145\n",
      "Training:\tepoch 4/80\tdiscr. gan loss 1.4518461227416992\tdiscr. class loss 4.26467752456665\tgen loss 0.03173941373825073\tsamples 1001/1145\n",
      "Training:\tepoch 4/80\taccuracy 0.0\n",
      "Test:\tepoch 4/80\tsamples 1/407\n",
      "Test:\tepoch 4/80\tsamples 51/407\n",
      "Test:\tepoch 4/80\tsamples 101/407\n",
      "Test:\tepoch 4/80\tsamples 151/407\n",
      "Test:\tepoch 4/80\tsamples 201/407\n",
      "Test:\tepoch 4/80\tsamples 251/407\n",
      "Test:\tepoch 4/80\tsamples 301/407\n",
      "Test:\tepoch 4/80\tsamples 351/407\n",
      "Test:\tepoch 4/80\tsamples 401/407\n",
      "Test:\tepoch 4/80\taccuracy 0.07114320835894283\n",
      "Training:\tepoch 5/80\tdiscr. gan loss 1.4312963485717773\tdiscr. class loss 0.0\tgen loss 0.014999128878116608\tsamples 1/1145\n",
      "Training:\tepoch 5/80\tdiscr. gan loss 1.1762850284576416\tdiscr. class loss 3.884373188018799\tgen loss 0.02359781228005886\tsamples 201/1145\n",
      "Training:\tepoch 5/80\tdiscr. gan loss 1.3416881561279297\tdiscr. class loss 0.0\tgen loss 0.02529359795153141\tsamples 401/1145\n",
      "Training:\tepoch 5/80\tdiscr. gan loss 1.3869128227233887\tdiscr. class loss 0.0\tgen loss 0.01049193274229765\tsamples 601/1145\n",
      "Training:\tepoch 5/80\tdiscr. gan loss 1.345660924911499\tdiscr. class loss 0.0\tgen loss 0.03624460846185684\tsamples 801/1145\n",
      "Training:\tepoch 5/80\tdiscr. gan loss 1.5830304622650146\tdiscr. class loss 4.292640209197998\tgen loss 0.012485065497457981\tsamples 1001/1145\n",
      "Training:\tepoch 5/80\taccuracy 0.0\n",
      "Test:\tepoch 5/80\tsamples 1/407\n",
      "Test:\tepoch 5/80\tsamples 51/407\n",
      "Test:\tepoch 5/80\tsamples 101/407\n",
      "Test:\tepoch 5/80\tsamples 151/407\n",
      "Test:\tepoch 5/80\tsamples 201/407\n",
      "Test:\tepoch 5/80\tsamples 251/407\n",
      "Test:\tepoch 5/80\tsamples 301/407\n",
      "Test:\tepoch 5/80\tsamples 351/407\n",
      "Test:\tepoch 5/80\tsamples 401/407\n",
      "Test:\tepoch 5/80\taccuracy 0.13030116779348494\n",
      "Training:\tepoch 6/80\tdiscr. gan loss 1.285435438156128\tdiscr. class loss 4.256315231323242\tgen loss 0.018680568784475327\tsamples 1/1145\n",
      "Training:\tepoch 6/80\tdiscr. gan loss 1.636690616607666\tdiscr. class loss 4.2763166427612305\tgen loss 0.0329572930932045\tsamples 201/1145\n",
      "Training:\tepoch 6/80\tdiscr. gan loss 1.4574410915374756\tdiscr. class loss 4.289712429046631\tgen loss 0.012290775775909424\tsamples 401/1145\n",
      "Training:\tepoch 6/80\tdiscr. gan loss 1.4068704843521118\tdiscr. class loss 4.062071323394775\tgen loss 0.011412044055759907\tsamples 601/1145\n",
      "Training:\tepoch 6/80\tdiscr. gan loss 1.3956081867218018\tdiscr. class loss 0.0\tgen loss 0.017891908064484596\tsamples 801/1145\n",
      "Training:\tepoch 6/80\tdiscr. gan loss 1.251287817955017\tdiscr. class loss 4.345641136169434\tgen loss 0.016474371775984764\tsamples 1001/1145\n",
      "Training:\tepoch 6/80\taccuracy 0.001\n",
      "Test:\tepoch 6/80\tsamples 1/407\n",
      "Test:\tepoch 6/80\tsamples 51/407\n",
      "Test:\tepoch 6/80\tsamples 101/407\n",
      "Test:\tepoch 6/80\tsamples 151/407\n",
      "Test:\tepoch 6/80\tsamples 201/407\n",
      "Test:\tepoch 6/80\tsamples 251/407\n",
      "Test:\tepoch 6/80\tsamples 301/407\n",
      "Test:\tepoch 6/80\tsamples 351/407\n",
      "Test:\tepoch 6/80\tsamples 401/407\n",
      "Test:\tepoch 6/80\taccuracy 0.1507759680393362\n",
      "Training:\tepoch 7/80\tdiscr. gan loss 1.380127191543579\tdiscr. class loss 0.0\tgen loss 0.014933406375348568\tsamples 1/1145\n",
      "Training:\tepoch 7/80\tdiscr. gan loss 0.9646270275115967\tdiscr. class loss 0.0\tgen loss 0.0767301619052887\tsamples 201/1145\n",
      "Training:\tepoch 7/80\tdiscr. gan loss 1.556936502456665\tdiscr. class loss 4.211158752441406\tgen loss 0.018765175715088844\tsamples 401/1145\n",
      "Training:\tepoch 7/80\tdiscr. gan loss 1.1909208297729492\tdiscr. class loss 0.0\tgen loss 0.048090893775224686\tsamples 601/1145\n",
      "Training:\tepoch 7/80\tdiscr. gan loss 0.9707345962524414\tdiscr. class loss 0.0\tgen loss 0.07330314815044403\tsamples 801/1145\n",
      "Training:\tepoch 7/80\tdiscr. gan loss 1.1081830263137817\tdiscr. class loss 0.0\tgen loss 0.03351292759180069\tsamples 1001/1145\n",
      "Training:\tepoch 7/80\taccuracy 0.005\n",
      "Test:\tepoch 7/80\tsamples 1/407\n",
      "Test:\tepoch 7/80\tsamples 51/407\n",
      "Test:\tepoch 7/80\tsamples 101/407\n",
      "Test:\tepoch 7/80\tsamples 151/407\n",
      "Test:\tepoch 7/80\tsamples 201/407\n",
      "Test:\tepoch 7/80\tsamples 251/407\n",
      "Test:\tepoch 7/80\tsamples 301/407\n",
      "Test:\tepoch 7/80\tsamples 351/407\n",
      "Test:\tepoch 7/80\tsamples 401/407\n",
      "Test:\tepoch 7/80\taccuracy 0.14347725875845113\n",
      "Training:\tepoch 8/80\tdiscr. gan loss 1.1025747060775757\tdiscr. class loss 4.339103698730469\tgen loss 0.13049158453941345\tsamples 1/1145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\tepoch 8/80\tdiscr. gan loss 1.0863593816757202\tdiscr. class loss 4.153226852416992\tgen loss 0.08293669670820236\tsamples 201/1145\n",
      "Training:\tepoch 8/80\tdiscr. gan loss 0.576751708984375\tdiscr. class loss 3.59065318107605\tgen loss 0.17277255654335022\tsamples 401/1145\n",
      "Training:\tepoch 8/80\tdiscr. gan loss 0.9780498743057251\tdiscr. class loss 4.26659631729126\tgen loss 0.11318211257457733\tsamples 601/1145\n",
      "Training:\tepoch 8/80\tdiscr. gan loss 1.9672470092773438\tdiscr. class loss 0.0\tgen loss 0.0894852876663208\tsamples 801/1145\n",
      "Training:\tepoch 8/80\tdiscr. gan loss 0.3905446231365204\tdiscr. class loss 4.542189598083496\tgen loss 0.14731082320213318\tsamples 1001/1145\n",
      "Training:\tepoch 8/80\taccuracy 0.062\n",
      "Test:\tepoch 8/80\tsamples 1/407\n",
      "Test:\tepoch 8/80\tsamples 51/407\n",
      "Test:\tepoch 8/80\tsamples 101/407\n",
      "Test:\tepoch 8/80\tsamples 151/407\n",
      "Test:\tepoch 8/80\tsamples 201/407\n",
      "Test:\tepoch 8/80\tsamples 251/407\n",
      "Test:\tepoch 8/80\tsamples 301/407\n",
      "Test:\tepoch 8/80\tsamples 351/407\n",
      "Test:\tepoch 8/80\tsamples 401/407\n",
      "Test:\tepoch 8/80\taccuracy 0.22514597418561771\n",
      "Training:\tepoch 9/80\tdiscr. gan loss 1.0363423824310303\tdiscr. class loss 0.0\tgen loss 0.17795319855213165\tsamples 1/1145\n",
      "Training:\tepoch 9/80\tdiscr. gan loss 0.6837730407714844\tdiscr. class loss 4.5213141441345215\tgen loss 0.10576770454645157\tsamples 201/1145\n",
      "Training:\tepoch 9/80\tdiscr. gan loss 0.9020872116088867\tdiscr. class loss 0.0\tgen loss 0.10643420368432999\tsamples 401/1145\n",
      "Training:\tepoch 9/80\tdiscr. gan loss 2.1669085025787354\tdiscr. class loss 4.472639083862305\tgen loss 0.21161077916622162\tsamples 601/1145\n",
      "Training:\tepoch 9/80\tdiscr. gan loss 0.7291072607040405\tdiscr. class loss 0.0\tgen loss 0.1062176376581192\tsamples 801/1145\n",
      "Training:\tepoch 9/80\tdiscr. gan loss 0.8951916694641113\tdiscr. class loss 0.0\tgen loss 0.020868483930826187\tsamples 1001/1145\n",
      "Training:\tepoch 9/80\taccuracy 0.107\n",
      "Test:\tepoch 9/80\tsamples 1/407\n",
      "Test:\tepoch 9/80\tsamples 51/407\n",
      "Test:\tepoch 9/80\tsamples 101/407\n",
      "Test:\tepoch 9/80\tsamples 151/407\n",
      "Test:\tepoch 9/80\tsamples 201/407\n",
      "Test:\tepoch 9/80\tsamples 251/407\n",
      "Test:\tepoch 9/80\tsamples 301/407\n",
      "Test:\tepoch 9/80\tsamples 351/407\n",
      "Test:\tepoch 9/80\tsamples 401/407\n",
      "Test:\tepoch 9/80\taccuracy 0.28161493546404426\n",
      "Training:\tepoch 10/80\tdiscr. gan loss 1.7379553318023682\tdiscr. class loss 4.180741310119629\tgen loss 0.08569778501987457\tsamples 1/1145\n",
      "Training:\tepoch 10/80\tdiscr. gan loss 0.6480546593666077\tdiscr. class loss 5.7069878578186035\tgen loss 0.10665275901556015\tsamples 201/1145\n",
      "Training:\tepoch 10/80\tdiscr. gan loss 0.9267073273658752\tdiscr. class loss 4.032135963439941\tgen loss 0.06945063173770905\tsamples 401/1145\n",
      "Training:\tepoch 10/80\tdiscr. gan loss 0.32383331656455994\tdiscr. class loss 4.66702127456665\tgen loss 0.16850629448890686\tsamples 601/1145\n",
      "Training:\tepoch 10/80\tdiscr. gan loss 0.5598443746566772\tdiscr. class loss 4.453888416290283\tgen loss 0.165565624833107\tsamples 801/1145\n",
      "Training:\tepoch 10/80\tdiscr. gan loss 0.726631224155426\tdiscr. class loss 0.0\tgen loss 0.06538782268762589\tsamples 1001/1145\n",
      "Training:\tepoch 10/80\taccuracy 0.126\n",
      "Test:\tepoch 10/80\tsamples 1/407\n",
      "Test:\tepoch 10/80\tsamples 51/407\n",
      "Test:\tepoch 10/80\tsamples 101/407\n",
      "Test:\tepoch 10/80\tsamples 151/407\n",
      "Test:\tepoch 10/80\tsamples 201/407\n",
      "Test:\tepoch 10/80\tsamples 251/407\n",
      "Test:\tepoch 10/80\tsamples 301/407\n",
      "Test:\tepoch 10/80\tsamples 351/407\n",
      "Test:\tepoch 10/80\tsamples 401/407\n",
      "Test:\tepoch 10/80\taccuracy 0.3431161647203442\n",
      "Training:\tepoch 11/80\tdiscr. gan loss 0.25932225584983826\tdiscr. class loss 4.474321365356445\tgen loss 0.1671464741230011\tsamples 1/1145\n",
      "Training:\tepoch 11/80\tdiscr. gan loss 0.713098406791687\tdiscr. class loss 0.0\tgen loss 0.047685131430625916\tsamples 201/1145\n",
      "Training:\tepoch 11/80\tdiscr. gan loss 0.8902356624603271\tdiscr. class loss 4.294144630432129\tgen loss 0.09193915128707886\tsamples 401/1145\n",
      "Training:\tepoch 11/80\tdiscr. gan loss 1.0492397546768188\tdiscr. class loss 0.0\tgen loss 0.06302738934755325\tsamples 601/1145\n",
      "Training:\tepoch 11/80\tdiscr. gan loss 0.6411497592926025\tdiscr. class loss 3.8980393409729004\tgen loss 0.1475200355052948\tsamples 801/1145\n",
      "Training:\tepoch 11/80\tdiscr. gan loss 1.4560606479644775\tdiscr. class loss 3.156205177307129\tgen loss 0.10097874701023102\tsamples 1001/1145\n",
      "Training:\tepoch 11/80\taccuracy 0.11\n",
      "Test:\tepoch 11/80\tsamples 1/407\n",
      "Test:\tepoch 11/80\tsamples 51/407\n",
      "Test:\tepoch 11/80\tsamples 101/407\n",
      "Test:\tepoch 11/80\tsamples 151/407\n",
      "Test:\tepoch 11/80\tsamples 201/407\n",
      "Test:\tepoch 11/80\tsamples 251/407\n",
      "Test:\tepoch 11/80\tsamples 301/407\n",
      "Test:\tepoch 11/80\tsamples 351/407\n",
      "Test:\tepoch 11/80\tsamples 401/407\n",
      "Test:\tepoch 11/80\taccuracy 0.38237553779963124\n",
      "Training:\tepoch 12/80\tdiscr. gan loss 1.0084174871444702\tdiscr. class loss 3.2075037956237793\tgen loss 0.05890698358416557\tsamples 1/1145\n",
      "Training:\tepoch 12/80\tdiscr. gan loss 1.488633394241333\tdiscr. class loss 3.999037265777588\tgen loss 0.14713989198207855\tsamples 201/1145\n",
      "Training:\tepoch 12/80\tdiscr. gan loss 1.4157531261444092\tdiscr. class loss 5.30348539352417\tgen loss 0.06509431451559067\tsamples 401/1145\n",
      "Training:\tepoch 12/80\tdiscr. gan loss 0.43929165601730347\tdiscr. class loss 2.8995087146759033\tgen loss 0.18936797976493835\tsamples 601/1145\n",
      "Training:\tepoch 12/80\tdiscr. gan loss 1.6365872621536255\tdiscr. class loss 1.924424648284912\tgen loss 0.14312826097011566\tsamples 801/1145\n",
      "Training:\tepoch 12/80\tdiscr. gan loss 1.8691434860229492\tdiscr. class loss 0.0\tgen loss 0.08065332472324371\tsamples 1001/1145\n",
      "Training:\tepoch 12/80\taccuracy 0.125\n",
      "Test:\tepoch 12/80\tsamples 1/407\n",
      "Test:\tepoch 12/80\tsamples 51/407\n",
      "Test:\tepoch 12/80\tsamples 101/407\n",
      "Test:\tepoch 12/80\tsamples 151/407\n",
      "Test:\tepoch 12/80\tsamples 201/407\n",
      "Test:\tepoch 12/80\tsamples 251/407\n",
      "Test:\tepoch 12/80\tsamples 301/407\n",
      "Test:\tepoch 12/80\tsamples 351/407\n",
      "Test:\tepoch 12/80\tsamples 401/407\n",
      "Test:\tepoch 12/80\taccuracy 0.4008143822987093\n",
      "Training:\tepoch 13/80\tdiscr. gan loss 0.7679964303970337\tdiscr. class loss 3.474137306213379\tgen loss 0.06997934728860855\tsamples 1/1145\n",
      "Training:\tepoch 13/80\tdiscr. gan loss 0.6364147663116455\tdiscr. class loss 0.0\tgen loss 0.1395474225282669\tsamples 201/1145\n",
      "Training:\tepoch 13/80\tdiscr. gan loss 0.39427047967910767\tdiscr. class loss 3.8485074043273926\tgen loss 0.1138407364487648\tsamples 401/1145\n",
      "Training:\tepoch 13/80\tdiscr. gan loss 1.2961658239364624\tdiscr. class loss 0.0\tgen loss 0.035218238830566406\tsamples 601/1145\n",
      "Training:\tepoch 13/80\tdiscr. gan loss 0.9787549376487732\tdiscr. class loss 0.0\tgen loss 0.15457679331302643\tsamples 801/1145\n",
      "Training:\tepoch 13/80\tdiscr. gan loss 1.6869783401489258\tdiscr. class loss 0.0\tgen loss 0.10361100733280182\tsamples 1001/1145\n",
      "Training:\tepoch 13/80\taccuracy 0.149\n",
      "Test:\tepoch 13/80\tsamples 1/407\n",
      "Test:\tepoch 13/80\tsamples 51/407\n",
      "Test:\tepoch 13/80\tsamples 101/407\n",
      "Test:\tepoch 13/80\tsamples 151/407\n",
      "Test:\tepoch 13/80\tsamples 201/407\n",
      "Test:\tepoch 13/80\tsamples 251/407\n",
      "Test:\tepoch 13/80\tsamples 301/407\n",
      "Test:\tepoch 13/80\tsamples 351/407\n",
      "Test:\tepoch 13/80\tsamples 401/407\n",
      "Test:\tepoch 13/80\taccuracy 0.44614320835894283\n",
      "Training:\tepoch 14/80\tdiscr. gan loss 0.811306357383728\tdiscr. class loss 0.0\tgen loss 0.11136369407176971\tsamples 1/1145\n",
      "Training:\tepoch 14/80\tdiscr. gan loss 1.7785817384719849\tdiscr. class loss 3.0534656047821045\tgen loss 0.1411883682012558\tsamples 201/1145\n",
      "Training:\tepoch 14/80\tdiscr. gan loss 0.3286030888557434\tdiscr. class loss 4.540157318115234\tgen loss 0.14197415113449097\tsamples 401/1145\n",
      "Training:\tepoch 14/80\tdiscr. gan loss 0.8117997646331787\tdiscr. class loss 3.628199815750122\tgen loss 0.06994614005088806\tsamples 601/1145\n",
      "Training:\tepoch 14/80\tdiscr. gan loss 0.9426923394203186\tdiscr. class loss 0.0\tgen loss 0.049947503954172134\tsamples 801/1145\n",
      "Training:\tepoch 14/80\tdiscr. gan loss 0.29540759325027466\tdiscr. class loss 0.0\tgen loss 0.09146112948656082\tsamples 1001/1145\n",
      "Training:\tepoch 14/80\taccuracy 0.179\n",
      "Test:\tepoch 14/80\tsamples 1/407\n",
      "Test:\tepoch 14/80\tsamples 51/407\n",
      "Test:\tepoch 14/80\tsamples 101/407\n",
      "Test:\tepoch 14/80\tsamples 151/407\n",
      "Test:\tepoch 14/80\tsamples 201/407\n",
      "Test:\tepoch 14/80\tsamples 251/407\n",
      "Test:\tepoch 14/80\tsamples 301/407\n",
      "Test:\tepoch 14/80\tsamples 351/407\n",
      "Test:\tepoch 14/80\tsamples 401/407\n",
      "Test:\tepoch 14/80\taccuracy 0.47080516287645974\n",
      "Training:\tepoch 15/80\tdiscr. gan loss 1.6053266525268555\tdiscr. class loss 0.0\tgen loss 0.20294928550720215\tsamples 1/1145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\tepoch 15/80\tdiscr. gan loss 1.000860571861267\tdiscr. class loss 0.0\tgen loss 0.16740038990974426\tsamples 201/1145\n",
      "Training:\tepoch 15/80\tdiscr. gan loss 1.7243224382400513\tdiscr. class loss 0.0\tgen loss 0.08360873907804489\tsamples 401/1145\n",
      "Training:\tepoch 15/80\tdiscr. gan loss 0.5288548469543457\tdiscr. class loss 0.0\tgen loss 0.14135615527629852\tsamples 601/1145\n",
      "Training:\tepoch 15/80\tdiscr. gan loss 0.5237624645233154\tdiscr. class loss 2.9733119010925293\tgen loss 0.20047374069690704\tsamples 801/1145\n",
      "Training:\tepoch 15/80\tdiscr. gan loss 0.5544688105583191\tdiscr. class loss 0.0\tgen loss 0.18500655889511108\tsamples 1001/1145\n",
      "Training:\tepoch 15/80\taccuracy 0.206\n",
      "Test:\tepoch 15/80\tsamples 1/407\n",
      "Test:\tepoch 15/80\tsamples 51/407\n",
      "Test:\tepoch 15/80\tsamples 101/407\n",
      "Test:\tepoch 15/80\tsamples 151/407\n",
      "Test:\tepoch 15/80\tsamples 201/407\n",
      "Test:\tepoch 15/80\tsamples 251/407\n",
      "Test:\tepoch 15/80\tsamples 301/407\n",
      "Test:\tepoch 15/80\tsamples 351/407\n",
      "Test:\tepoch 15/80\tsamples 401/407\n",
      "Test:\tepoch 15/80\taccuracy 0.5142132759680393\n",
      "Training:\tepoch 16/80\tdiscr. gan loss 0.6026597619056702\tdiscr. class loss 0.0\tgen loss 0.1451699584722519\tsamples 1/1145\n",
      "Training:\tepoch 16/80\tdiscr. gan loss 0.6669204235076904\tdiscr. class loss 0.0\tgen loss 0.09909531474113464\tsamples 201/1145\n",
      "Training:\tepoch 16/80\tdiscr. gan loss 1.7879953384399414\tdiscr. class loss 3.9049744606018066\tgen loss 0.207717627286911\tsamples 401/1145\n",
      "Training:\tepoch 16/80\tdiscr. gan loss 1.091320276260376\tdiscr. class loss 3.0126395225524902\tgen loss 0.07905350625514984\tsamples 601/1145\n",
      "Training:\tepoch 16/80\tdiscr. gan loss 1.1561646461486816\tdiscr. class loss 0.0\tgen loss 0.10611746460199356\tsamples 801/1145\n",
      "Training:\tepoch 16/80\tdiscr. gan loss 0.6049361228942871\tdiscr. class loss 4.093478202819824\tgen loss 0.1496514081954956\tsamples 1001/1145\n",
      "Training:\tepoch 16/80\taccuracy 0.244\n",
      "Test:\tepoch 16/80\tsamples 1/407\n",
      "Test:\tepoch 16/80\tsamples 51/407\n",
      "Test:\tepoch 16/80\tsamples 101/407\n",
      "Test:\tepoch 16/80\tsamples 151/407\n",
      "Test:\tepoch 16/80\tsamples 201/407\n",
      "Test:\tepoch 16/80\tsamples 251/407\n",
      "Test:\tepoch 16/80\tsamples 301/407\n",
      "Test:\tepoch 16/80\tsamples 351/407\n",
      "Test:\tepoch 16/80\tsamples 401/407\n",
      "Test:\tepoch 16/80\taccuracy 0.5377612169637369\n",
      "Training:\tepoch 17/80\tdiscr. gan loss 1.1996740102767944\tdiscr. class loss 2.838407516479492\tgen loss 0.11767876148223877\tsamples 1/1145\n",
      "Training:\tepoch 17/80\tdiscr. gan loss 0.4920027256011963\tdiscr. class loss 0.0\tgen loss 0.16798198223114014\tsamples 201/1145\n",
      "Training:\tepoch 17/80\tdiscr. gan loss 1.6293591260910034\tdiscr. class loss 0.0\tgen loss 0.11111552268266678\tsamples 401/1145\n",
      "Training:\tepoch 17/80\tdiscr. gan loss 0.7002066969871521\tdiscr. class loss 3.433454990386963\tgen loss 0.13589319586753845\tsamples 601/1145\n",
      "Training:\tepoch 17/80\tdiscr. gan loss 0.5958058834075928\tdiscr. class loss 0.0\tgen loss 0.12914448976516724\tsamples 801/1145\n",
      "Training:\tepoch 17/80\tdiscr. gan loss 1.506525993347168\tdiscr. class loss 3.0901498794555664\tgen loss 0.11817225068807602\tsamples 1001/1145\n",
      "Training:\tepoch 17/80\taccuracy 0.289\n",
      "Test:\tepoch 17/80\tsamples 1/407\n",
      "Test:\tepoch 17/80\tsamples 51/407\n",
      "Test:\tepoch 17/80\tsamples 101/407\n",
      "Test:\tepoch 17/80\tsamples 151/407\n",
      "Test:\tepoch 17/80\tsamples 201/407\n",
      "Test:\tepoch 17/80\tsamples 251/407\n",
      "Test:\tepoch 17/80\tsamples 301/407\n",
      "Test:\tepoch 17/80\tsamples 351/407\n",
      "Test:\tepoch 17/80\tsamples 401/407\n",
      "Test:\tepoch 17/80\taccuracy 0.5452135832821143\n",
      "Training:\tepoch 18/80\tdiscr. gan loss 0.3624921143054962\tdiscr. class loss 0.0\tgen loss 0.13605254888534546\tsamples 1/1145\n",
      "Training:\tepoch 18/80\tdiscr. gan loss 2.2452845573425293\tdiscr. class loss 0.0\tgen loss 0.1142372414469719\tsamples 201/1145\n",
      "Training:\tepoch 18/80\tdiscr. gan loss 0.2960098385810852\tdiscr. class loss 2.7030277252197266\tgen loss 0.18462781608104706\tsamples 401/1145\n",
      "Training:\tepoch 18/80\tdiscr. gan loss 1.014603853225708\tdiscr. class loss 0.0\tgen loss 0.1369338482618332\tsamples 601/1145\n",
      "Training:\tepoch 18/80\tdiscr. gan loss 0.38148459792137146\tdiscr. class loss 0.0\tgen loss 0.12322714924812317\tsamples 801/1145\n",
      "Training:\tepoch 18/80\tdiscr. gan loss 0.7295452952384949\tdiscr. class loss 2.4014732837677\tgen loss 0.2695125937461853\tsamples 1001/1145\n",
      "Training:\tepoch 18/80\taccuracy 0.317\n",
      "Test:\tepoch 18/80\tsamples 1/407\n",
      "Test:\tepoch 18/80\tsamples 51/407\n",
      "Test:\tepoch 18/80\tsamples 101/407\n",
      "Test:\tepoch 18/80\tsamples 151/407\n",
      "Test:\tepoch 18/80\tsamples 201/407\n",
      "Test:\tepoch 18/80\tsamples 251/407\n",
      "Test:\tepoch 18/80\tsamples 301/407\n",
      "Test:\tepoch 18/80\tsamples 351/407\n",
      "Test:\tepoch 18/80\tsamples 401/407\n",
      "Test:\tepoch 18/80\taccuracy 0.581361401352182\n",
      "Training:\tepoch 19/80\tdiscr. gan loss 0.5532473921775818\tdiscr. class loss 0.5804768204689026\tgen loss 0.18406136333942413\tsamples 1/1145\n",
      "Training:\tepoch 19/80\tdiscr. gan loss 0.5026048421859741\tdiscr. class loss 4.037604808807373\tgen loss 0.13684263825416565\tsamples 201/1145\n",
      "Training:\tepoch 19/80\tdiscr. gan loss 0.3003038167953491\tdiscr. class loss 0.0\tgen loss 0.1110152080655098\tsamples 401/1145\n",
      "Training:\tepoch 19/80\tdiscr. gan loss 0.4755873680114746\tdiscr. class loss 0.13997222483158112\tgen loss 0.16011910140514374\tsamples 601/1145\n",
      "Training:\tepoch 19/80\tdiscr. gan loss 1.5340358018875122\tdiscr. class loss 3.268092632293701\tgen loss 0.23673173785209656\tsamples 801/1145\n",
      "Training:\tepoch 19/80\tdiscr. gan loss 0.597630500793457\tdiscr. class loss 3.764672040939331\tgen loss 0.1536112278699875\tsamples 1001/1145\n",
      "Training:\tepoch 19/80\taccuracy 0.336\n",
      "Test:\tepoch 19/80\tsamples 1/407\n",
      "Test:\tepoch 19/80\tsamples 51/407\n",
      "Test:\tepoch 19/80\tsamples 101/407\n",
      "Test:\tepoch 19/80\tsamples 151/407\n",
      "Test:\tepoch 19/80\tsamples 201/407\n",
      "Test:\tepoch 19/80\tsamples 251/407\n",
      "Test:\tepoch 19/80\tsamples 301/407\n",
      "Test:\tepoch 19/80\tsamples 351/407\n",
      "Test:\tepoch 19/80\tsamples 401/407\n",
      "Test:\tepoch 19/80\taccuracy 0.5933466502765826\n",
      "Training:\tepoch 20/80\tdiscr. gan loss 1.2089673280715942\tdiscr. class loss 2.8750455379486084\tgen loss 0.14475864171981812\tsamples 1/1145\n",
      "Training:\tepoch 20/80\tdiscr. gan loss 0.995025634765625\tdiscr. class loss 0.0\tgen loss 0.2044532150030136\tsamples 201/1145\n",
      "Training:\tepoch 20/80\tdiscr. gan loss 0.4502791166305542\tdiscr. class loss 0.0\tgen loss 0.1359448879957199\tsamples 401/1145\n",
      "Training:\tepoch 20/80\tdiscr. gan loss 1.1970058679580688\tdiscr. class loss 3.249155282974243\tgen loss 0.26065415143966675\tsamples 601/1145\n",
      "Training:\tepoch 20/80\tdiscr. gan loss 1.2033188343048096\tdiscr. class loss 0.0\tgen loss 0.14515289664268494\tsamples 801/1145\n",
      "Training:\tepoch 20/80\tdiscr. gan loss 0.8238485455513\tdiscr. class loss 4.3040595054626465\tgen loss 0.08903554081916809\tsamples 1001/1145\n",
      "Training:\tepoch 20/80\taccuracy 0.361\n",
      "Test:\tepoch 20/80\tsamples 1/407\n",
      "Test:\tepoch 20/80\tsamples 51/407\n",
      "Test:\tepoch 20/80\tsamples 101/407\n",
      "Test:\tepoch 20/80\tsamples 151/407\n",
      "Test:\tepoch 20/80\tsamples 201/407\n",
      "Test:\tepoch 20/80\tsamples 251/407\n",
      "Test:\tepoch 20/80\tsamples 301/407\n",
      "Test:\tepoch 20/80\tsamples 351/407\n",
      "Test:\tepoch 20/80\tsamples 401/407\n",
      "Test:\tepoch 20/80\taccuracy 0.5885064535955746\n",
      "Training:\tepoch 21/80\tdiscr. gan loss 1.258629322052002\tdiscr. class loss 2.0125958919525146\tgen loss 0.09359045326709747\tsamples 1/1145\n",
      "Training:\tepoch 21/80\tdiscr. gan loss 0.3976702392101288\tdiscr. class loss 3.6526238918304443\tgen loss 0.1988922357559204\tsamples 201/1145\n",
      "Training:\tepoch 21/80\tdiscr. gan loss 0.5548442602157593\tdiscr. class loss 5.140216827392578\tgen loss 0.1453600972890854\tsamples 401/1145\n",
      "Training:\tepoch 21/80\tdiscr. gan loss 0.8970401287078857\tdiscr. class loss 1.0554637908935547\tgen loss 0.06841237843036652\tsamples 601/1145\n",
      "Training:\tepoch 21/80\tdiscr. gan loss 0.47910264134407043\tdiscr. class loss 0.0\tgen loss 0.14673611521720886\tsamples 801/1145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-84:\n",
      "Process Process-83:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vetal/anaconda3/envs/aind2/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vetal/anaconda3/envs/aind2/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vetal/anaconda3/envs/aind2/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vetal/anaconda3/envs/aind2/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vetal/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/vetal/anaconda3/envs/aind2/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/vetal/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/vetal/anaconda3/envs/aind2/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/vetal/anaconda3/envs/aind2/lib/python3.5/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/vetal/anaconda3/envs/aind2/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/vetal/anaconda3/envs/aind2/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/vetal/anaconda3/envs/aind2/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-690023f6a75b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-cb873f7e2ce0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;31m# call netD again to do backprop for netG here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_data_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# Here we set `g_loss` to the \"feature matching\" loss invented by Tim Salimans at OpenAI.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-59c072bbcd9f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mdata_parallel\u001b[0;34m(module, inputs, device_ids, output_device, dim, module_kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0mused_device_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused_device_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 277\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if os.path.exists(settings.best_netG_filename) and os.path.exists(settings.best_netD_filename):\n",
    "    solver.load_model(settings.best_netG_filename, settings.best_netD_filename)\n",
    "    solver.test(1, 1)\n",
    "else:\n",
    "    solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
